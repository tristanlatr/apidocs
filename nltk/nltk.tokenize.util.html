<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.tokenize.util</title>
    <meta name="generator" content="pydoctor 22.2.2.dev0"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
        <span class="navbar-brand">
          <a href="http://www.nltk.org/" class="projecthome">nltk-3.6.2</a> <a href="index.html">API Documentation</a>
        </span>

        <a href="moduleIndex.html">
          Modules
        </a>

        <a href="classIndex.html">
          Classes
        </a>

        <a href="nameIndex.html">
          Names
        </a>

        <div id="search-box-container">
          <div class="input-group">
            <input id="search-box" type="search" name="search-query" placeholder="Search..." aria-label="Search" minlength="2" class="form-control" autocomplete="off" />
            
            <span class="input-group-btn">
              <a style="display: none;" class="btn btn-default" id="search-clear-button" title="Clear" onclick="clearSearch()"><img src="fonts/x-circle.svg" alt="Clear" /></a>
              <a class="btn btn-default" id="search-help-button" title="Help" onclick="toggleSearchHelpText()"><img src="fonts/info.svg" alt="Help" /></a>
            </span>
          </div>
        </div>

      </div>

      <div id="search-results-container" style="display: none;">
        <div>
          <span class="label label-default" id="search-docstrings-button">
            <a title="Search in docstrings" onclick="toggleSearchInDocstrings()">search in docstrings</a></span>
        </div>

        <noscript>
            <h1>Cannot search: JavaScript is not supported/enabled in your browser.</h1>
        </noscript>

        <div class="hint" id="search-help-box">
          <p class="rst-last">
      
            Search bar offers the following options:
            <ul>   
                <li>
                  <strong>Term presence.</strong> The below example searches for documents that 
                    must contain “foo”, might contain “bar” and must not contain “baz”: <code>+foo bar -baz</code>
                </li> 

                <li>
                  <strong>Wildcards.</strong> The below example searches for documents with words beginning with “foo”: <code>foo*</code>
                </li> 

                <li>
                  <strong>Search in specific fields.</strong> The following search matches all objects 
                  in "twisted.mail" that matches “search”: <code>+qname:twisted.mail.* +search</code>

                  <p>
                    Possible fields: 'name', 'qname' (fully qualified name), 'docstring', and 'kind'.
                    Last two fields are only applicable if "search in docstrings" is enabled.
                  </p>
                </li>

                <li>
                  <strong>Fuzzy matches.</strong> The following search matches all documents 
                  that have a word within 1 edit distance of “foo”: <code>foo~1</code>
                </li>
            </ul>

          </p>
        </div>

        <div id="search-status"> </div>
        
        <div class="warning" id="search-warn-box" style="display: none;">
          <p class="rst-last"><span id="search-warn"></span></p>
        </div>

        <table id="search-results">
          <!-- Filled dynamically by JS -->
        </table>
        
        <div style="margin-top: 8px;">
          <p>Results provided by <a href="https://lunrjs.com">Lunr.js</a></p>
        </div>
      </div>

    </div>
  </div>

  <script src="ajax.js" type="text/javascript"></script>
  <script src="search.js" type="text/javascript"></script>

</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="module"><code><code><a href="index.html" class="internal-link">nltk</a></code><wbr></wbr>.<code><a href="nltk.tokenize.html" class="internal-link" title="nltk.tokenize">tokenize</a></code><wbr></wbr>.<code><a href="nltk.tokenize.util.html" class="internal-link" title="nltk.tokenize.util">util</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        module documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p class="undocumented">Undocumented</p></div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1863">
  
  
  <tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.tokenize.util.CJKChars.html" class="internal-link" title="nltk.tokenize.util.CJKChars">​CJKChars</a></code></td>
    <td>An object that enumerates the code points of the CJK characters as listed on <a class="rst-reference external" href="http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane" target="_top">http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane</a></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#align_tokens" class="internal-link" title="nltk.tokenize.util.align_tokens">align​_tokens</a></code></td>
    <td>This module attempt to find the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt> tuples, given the tokens and also the source string.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#is_cjk" class="internal-link" title="nltk.tokenize.util.is_cjk">is​_cjk</a></code></td>
    <td>Python port of Moses' code to check for CJK character.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#regexp_span_tokenize" class="internal-link" title="nltk.tokenize.util.regexp_span_tokenize">regexp​_span​_tokenize</a></code></td>
    <td>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt> tuples, by splitting the string at each successive match of <em>regexp</em>.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#spans_to_relative" class="internal-link" title="nltk.tokenize.util.spans_to_relative">spans​_to​_relative</a></code></td>
    <td>Return a sequence of relative spans, given a sequence of spans.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#string_span_tokenize" class="internal-link" title="nltk.tokenize.util.string_span_tokenize">string​_span​_tokenize</a></code></td>
    <td>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt> tuples, by splitting the string at each occurrence of <em>sep</em>.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#xml_escape" class="internal-link" title="nltk.tokenize.util.xml_escape">xml​_escape</a></code></td>
    <td>This function transforms the input text into an "escaped" version suitable for well-formed XML formatting.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#xml_unescape" class="internal-link" title="nltk.tokenize.util.xml_unescape">xml​_unescape</a></code></td>
    <td>This function transforms the "escaped" version suitable for well-formed XML formatting into humanly-readable string.</td>
  </tr>
</table>
        

          
      </div>

      <div id="childList">

        <div class="basefunction">
  
  
  <a name="nltk.tokenize.util.align_tokens">
    
  </a>
  <a name="align_tokens">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">align_tokens</span>(tokens, sentence):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L257">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>This module attempt to find the offsets of the tokens in <em>s</em>, as a sequence
of <tt class="rst-docutils literal">(start, end)</tt> tuples, given the tokens and also the source string.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> TreebankWordTokenizer
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> align_tokens
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-builtin">str</span>(<span class="py-string">"The plane, bound for St Petersburg, crashed in Egypt's "</span>
<span class="py-more">... </span><span class="py-string">"Sinai desert just 23 minutes after take-off from Sharm el-Sheikh "</span>
<span class="py-more">... </span><span class="py-string">"on Saturday."</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>tokens = TreebankWordTokenizer().tokenize(s)
<span class="py-prompt">&gt;&gt;&gt; </span>expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
<span class="py-more">... </span>(24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
<span class="py-more">... </span>(55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
<span class="py-more">... </span>(90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
<span class="py-more">... </span>(123, 131), (131, 132)]
<span class="py-prompt">&gt;&gt;&gt; </span>output = <span class="py-builtin">list</span>(align_tokens(tokens, s))
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">len</span>(tokens) == <span class="py-builtin">len</span>(expected) == <span class="py-builtin">len</span>(output)  <span class="py-comment"># Check that length of tokens and tuples are the same.</span>
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>expected == <span class="py-builtin">list</span>(align_tokens(tokens, s))  <span class="py-comment"># Check that the output is as expected.</span>
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>tokens == [s[start:end] <span class="py-keyword">for</span> start, end <span class="py-keyword">in</span> output]  <span class="py-comment"># Check that the slices of the string corresponds to the tokens.</span>
<span class="py-output">True</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">tokens:</span>list(str)</td><td class="fieldArgDesc">The list of strings that are the result of tokenization</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">sentence:</span>str</td><td class="fieldArgDesc">The original string</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">list(tuple(int,int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.is_cjk">
    
  </a>
  <a name="is_cjk">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">is_cjk</span>(character):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L162">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Python port of Moses' code to check for CJK character.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>CJKChars().ranges
<span class="py-output">[(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]</span>
<span class="py-prompt">&gt;&gt;&gt; </span>is_cjk(u<span class="py-string">'㏾'</span>)
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>is_cjk(u<span class="py-string">'﹟'</span>)
<span class="py-output">False</span>
</pre><table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">character:</span>char</td><td class="fieldArgDesc">The character that needs to be checked.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td colspan="2">bool</td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.regexp_span_tokenize">
    
  </a>
  <a name="regexp_span_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">regexp_span_tokenize</span>(s, regexp):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L47">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt>
tuples, by splitting the string at each successive match of <em>regexp</em>.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> regexp_span_tokenize
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">'''Good muffins cost $3.88\nin New York.  Please buy me</span>
<span class="py-more">... </span><span class="py-string">two of them.\n\nThanks.'''</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(regexp_span_tokenize(s, r<span class="py-string">'\s'</span>))
<span class="py-output">[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),</span>
<span class="py-output">(38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">s:</span>str</td><td class="fieldArgDesc">the string to be tokenized</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">regexp:</span>str</td><td class="fieldArgDesc">regular expression that matches token separators (must not be empty)</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">iter(tuple(int, int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.spans_to_relative">
    
  </a>
  <a name="spans_to_relative">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">spans_to_relative</span>(spans):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L74">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Return a sequence of relative spans, given a sequence of spans.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> WhitespaceTokenizer
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> spans_to_relative
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">'''Good muffins cost $3.88\nin New York.  Please buy me</span>
<span class="py-more">... </span><span class="py-string">two of them.\n\nThanks.'''</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
<span class="py-output">[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),</span>
<span class="py-output">(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">spans:</span>iter(tuple(int, int))</td><td class="fieldArgDesc">a sequence of (start, end) offsets of the tokens</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">iter(tuple(int, int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.string_span_tokenize">
    
  </a>
  <a name="string_span_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">string_span_tokenize</span>(s, sep):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L13">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt>
tuples, by splitting the string at each occurrence of <em>sep</em>.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> string_span_tokenize
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">'''Good muffins cost $3.88\nin New York.  Please buy me</span>
<span class="py-more">... </span><span class="py-string">two of them.\n\nThanks.'''</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(string_span_tokenize(s, <span class="py-string">" "</span>))
<span class="py-output">[(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),</span>
<span class="py-output">(38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">s:</span>str</td><td class="fieldArgDesc">the string to be tokenized</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">sep:</span>str</td><td class="fieldArgDesc">the token separator</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">iter(tuple(int, int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.xml_escape">
    
  </a>
  <a name="xml_escape">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">xml_escape</span>(text):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L194">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>This function transforms the input text into an "escaped" version suitable
for well-formed XML formatting.</p>
<p>Note that the default xml.sax.saxutils.escape() function don't escape
some characters that Moses does so we have to manually add them to the
entities dictionary.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>input_str = <span class="py-string">''')| &amp; &lt; &gt; ' " ] ['''</span>
<span class="py-prompt">&gt;&gt;&gt; </span>expected_output =  <span class="py-string">''')| &amp;amp; &amp;lt; &amp;gt; ' " ] ['''</span>
<span class="py-prompt">&gt;&gt;&gt; </span>escape(input_str) == expected_output
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>xml_escape(input_str)
<span class="py-output">')&amp;#124; &amp;amp; &amp;lt; &amp;gt; &amp;apos; &amp;quot; &amp;#93; &amp;#91;'</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">text:</span>str</td><td class="fieldArgDesc">The text that needs to be escaped.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">str</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.xml_unescape">
    
  </a>
  <a name="xml_unescape">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">xml_unescape</span>(text):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/tokenize/util.py#L226">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>This function transforms the "escaped" version suitable
for well-formed XML formatting into humanly-readable string.</p>
<p>Note that the default xml.sax.saxutils.unescape() function don't unescape
some characters that Moses does so we have to manually add them to the
entities dictionary.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> xml.sax.saxutils <span class="py-keyword">import</span> unescape
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">')&amp;#124; &amp;amp; &amp;lt; &amp;gt; &amp;apos; &amp;quot; &amp;#93; &amp;#91;'</span>
<span class="py-prompt">&gt;&gt;&gt; </span>expected = <span class="py-string">''')| &amp; &lt; &gt; ' " ] ['''</span>
<span class="py-prompt">&gt;&gt;&gt; </span>xml_unescape(s) == expected
<span class="py-output">True</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">text:</span>str</td><td class="fieldArgDesc">The text that needs to be unescaped.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">str</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div>

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="http://www.nltk.org/" class="projecthome">nltk-3.6.2</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    22.2.2.dev0 at 2022-03-04 05:57:24.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>