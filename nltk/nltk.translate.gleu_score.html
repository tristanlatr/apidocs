<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.translate.gleu_score</title>
    <meta name="generator" content="pydoctor 22.2.2.dev0"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
        <span class="navbar-brand">
          <a href="http://www.nltk.org/" class="projecthome">nltk-3.6.2</a> <a href="index.html">API Documentation</a>
        </span>

        <a href="moduleIndex.html">
          Modules
        </a>

        <a href="classIndex.html">
          Classes
        </a>

        <a href="nameIndex.html">
          Names
        </a>

        <div id="search-box-container">
          <div class="input-group">
            <input id="search-box" type="search" name="search-query" placeholder="Search..." aria-label="Search" minlength="2" class="form-control" autocomplete="off" />
            
            <span class="input-group-btn">
              <a style="display: none;" class="btn btn-default" id="search-clear-button" title="Clear" onclick="clearSearch()"><img src="fonts/x-circle.svg" alt="Clear" /></a>
              <a class="btn btn-default" id="search-help-button" title="Help" onclick="toggleSearchHelpText()"><img src="fonts/info.svg" alt="Help" /></a>
            </span>
          </div>
        </div>

      </div>

      <div id="search-results-container" style="display: none;">
        <div>
          <span class="label label-default" id="search-docstrings-button">
            <a title="Search in docstrings" onclick="toggleSearchInDocstrings()">search in docstrings</a></span>
        </div>

        <noscript>
            <h1>Cannot search: JavaScript is not supported/enabled in your browser.</h1>
        </noscript>

        <div class="hint" id="search-help-box">
          <p class="rst-last">
      
            Search bar offers the following options:
            <ul>   
                <li>
                  <strong>Term presence.</strong> The below example searches for documents that 
                    must contain “foo”, might contain “bar” and must not contain “baz”: <code>+foo bar -baz</code>
                </li> 

                <li>
                  <strong>Wildcards.</strong> The below example searches for documents with words beginning with “foo”: <code>foo*</code>
                </li> 

                <li>
                  <strong>Search in specific fields.</strong> The following search matches all objects 
                  in "twisted.mail" that matches “search”: <code>+qname:twisted.mail.* +search</code>

                  <p>
                    Possible fields: 'name', 'qname' (fully qualified name), 'docstring', and 'kind'.
                    Last two fields are only applicable if "search in docstrings" is enabled.
                  </p>
                </li>

                <li>
                  <strong>Fuzzy matches.</strong> The following search matches all documents 
                  that have a word within 1 edit distance of “foo”: <code>foo~1</code>
                </li>
            </ul>

          </p>
        </div>

        <div id="search-status"> </div>
        
        <div class="warning" id="search-warn-box" style="display: none;">
          <p class="rst-last"><span id="search-warn"></span></p>
        </div>

        <table id="search-results">
          <!-- Filled dynamically by JS -->
        </table>
        
        <div style="margin-top: 8px;">
          <p>Results provided by <a href="https://lunrjs.com">Lunr.js</a></p>
        </div>
      </div>

    </div>
  </div>

  <script src="ajax.js" type="text/javascript"></script>
  <script src="search.js" type="text/javascript"></script>

</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="module"><code><code><a href="index.html" class="internal-link">nltk</a></code><wbr></wbr>.<code><a href="nltk.translate.html" class="internal-link" title="nltk.translate">translate</a></code><wbr></wbr>.<code><a href="nltk.translate.gleu_score.html" class="internal-link" title="nltk.translate.gleu_score">gleu_score</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        module documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/nltk/nltk/tree/3.6.2/nltk/translate/gleu_score.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div>GLEU score implementation.</div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1882">
  
  
  <tr class="function">
    
    <td>Function</td>
    <td><code><a href="#corpus_gleu" class="internal-link" title="nltk.translate.gleu_score.corpus_gleu">corpus​_gleu</a></code></td>
    <td>Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all the hypotheses and their respective references.</td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#sentence_gleu" class="internal-link" title="nltk.translate.gleu_score.sentence_gleu">sentence​_gleu</a></code></td>
    <td>Calculates the sentence level GLEU (Google-BLEU) score described in</td>
  </tr>
</table>
        

          
      </div>

      <div id="childList">

        <div class="basefunction">
  
  
  <a name="nltk.translate.gleu_score.corpus_gleu">
    
  </a>
  <a name="corpus_gleu">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">corpus_gleu</span>(list_of_references, hypotheses, min_len=1, max_len=4):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/translate/gleu_score.py#L87">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all
the hypotheses and their respective references.</p>
<p>Instead of averaging the sentence level GLEU scores (i.e. macro-average
precision), Wu et al. (2016) sum up the matching tokens and the max of
hypothesis and reference tokens for each sentence, then compute using the
aggregate values.</p>
<dl class="rst-docutils">
<dt>From Mike Schuster (via email):</dt>
<dd><dl class="rst-first rst-last rst-docutils">
<dt>"For the corpus, we just add up the two statistics n_match and</dt>
<dd>n_all = max(n_all_output, n_all_target) for all sentences, then
calculate gleu_score = n_match / n_all, so it is not just a mean of
the sentence gleu scores (in our case, longer sentences count more,
which I think makes sense as they are more difficult to translate)."</dd>
</dl>
</dd>
</dl>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>hyp1 = [<span class="py-string">'It'</span>, <span class="py-string">'is'</span>, <span class="py-string">'a'</span>, <span class="py-string">'guide'</span>, <span class="py-string">'to'</span>, <span class="py-string">'action'</span>, <span class="py-string">'which'</span>,
<span class="py-more">... </span>        <span class="py-string">'ensures'</span>, <span class="py-string">'that'</span>, <span class="py-string">'the'</span>, <span class="py-string">'military'</span>, <span class="py-string">'always'</span>,
<span class="py-more">... </span>        <span class="py-string">'obeys'</span>, <span class="py-string">'the'</span>, <span class="py-string">'commands'</span>, <span class="py-string">'of'</span>, <span class="py-string">'the'</span>, <span class="py-string">'party'</span>]
<span class="py-prompt">&gt;&gt;&gt; </span>ref1a = [<span class="py-string">'It'</span>, <span class="py-string">'is'</span>, <span class="py-string">'a'</span>, <span class="py-string">'guide'</span>, <span class="py-string">'to'</span>, <span class="py-string">'action'</span>, <span class="py-string">'that'</span>,
<span class="py-more">... </span>         <span class="py-string">'ensures'</span>, <span class="py-string">'that'</span>, <span class="py-string">'the'</span>, <span class="py-string">'military'</span>, <span class="py-string">'will'</span>, <span class="py-string">'forever'</span>,
<span class="py-more">... </span>         <span class="py-string">'heed'</span>, <span class="py-string">'Party'</span>, <span class="py-string">'commands'</span>]
<span class="py-prompt">&gt;&gt;&gt; </span>ref1b = [<span class="py-string">'It'</span>, <span class="py-string">'is'</span>, <span class="py-string">'the'</span>, <span class="py-string">'guiding'</span>, <span class="py-string">'principle'</span>, <span class="py-string">'which'</span>,
<span class="py-more">... </span>         <span class="py-string">'guarantees'</span>, <span class="py-string">'the'</span>, <span class="py-string">'military'</span>, <span class="py-string">'forces'</span>, <span class="py-string">'always'</span>,
<span class="py-more">... </span>         <span class="py-string">'being'</span>, <span class="py-string">'under'</span>, <span class="py-string">'the'</span>, <span class="py-string">'command'</span>, <span class="py-string">'of'</span>, <span class="py-string">'the'</span>, <span class="py-string">'Party'</span>]
<span class="py-prompt">&gt;&gt;&gt; </span>ref1c = [<span class="py-string">'It'</span>, <span class="py-string">'is'</span>, <span class="py-string">'the'</span>, <span class="py-string">'practical'</span>, <span class="py-string">'guide'</span>, <span class="py-string">'for'</span>, <span class="py-string">'the'</span>,
<span class="py-more">... </span>         <span class="py-string">'army'</span>, <span class="py-string">'always'</span>, <span class="py-string">'to'</span>, <span class="py-string">'heed'</span>, <span class="py-string">'the'</span>, <span class="py-string">'directions'</span>,
<span class="py-more">... </span>         <span class="py-string">'of'</span>, <span class="py-string">'the'</span>, <span class="py-string">'party'</span>]</pre><pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>hyp2 = [<span class="py-string">'he'</span>, <span class="py-string">'read'</span>, <span class="py-string">'the'</span>, <span class="py-string">'book'</span>, <span class="py-string">'because'</span>, <span class="py-string">'he'</span>, <span class="py-string">'was'</span>,
<span class="py-more">... </span>        <span class="py-string">'interested'</span>, <span class="py-string">'in'</span>, <span class="py-string">'world'</span>, <span class="py-string">'history'</span>]
<span class="py-prompt">&gt;&gt;&gt; </span>ref2a = [<span class="py-string">'he'</span>, <span class="py-string">'was'</span>, <span class="py-string">'interested'</span>, <span class="py-string">'in'</span>, <span class="py-string">'world'</span>, <span class="py-string">'history'</span>,
<span class="py-more">... </span>         <span class="py-string">'because'</span>, <span class="py-string">'he'</span>, <span class="py-string">'read'</span>, <span class="py-string">'the'</span>, <span class="py-string">'book'</span>]</pre><pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]
<span class="py-prompt">&gt;&gt;&gt; </span>hypotheses = [hyp1, hyp2]
<span class="py-prompt">&gt;&gt;&gt; </span>corpus_gleu(list_of_references, hypotheses) <span class="py-comment"># doctest: +ELLIPSIS</span>
<span class="py-output">0.5673...</span>
</pre><p>The example below show that corpus_gleu() is different from averaging
sentence_gleu() for hypotheses</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>score1 = sentence_gleu([ref1a], hyp1)
<span class="py-prompt">&gt;&gt;&gt; </span>score2 = sentence_gleu([ref2a], hyp2)
<span class="py-prompt">&gt;&gt;&gt; </span>(score1 + score2) / 2 <span class="py-comment"># doctest: +ELLIPSIS</span>
<span class="py-output">0.6144...</span>
</pre><table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">list​_of​_references:</span>list(list(list(str)))</td><td class="fieldArgDesc">a list of reference sentences, w.r.t. hypotheses</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">hypotheses:</span>list(list(str))</td><td class="fieldArgDesc">a list of hypothesis sentences</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">min​_len:</span>int</td><td class="fieldArgDesc">The minimum order of n-gram this function should extract.</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">max​_len:</span>int</td><td class="fieldArgDesc">The maximum order of n-gram this function should extract.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">float</td><td class="fieldArgDesc">The corpus-level GLEU score.</td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.translate.gleu_score.sentence_gleu">
    
  </a>
  <a name="sentence_gleu">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">sentence_gleu</span>(references, hypothesis, min_len=1, max_len=4):
    <a class="sourceLink" href="https://github.com/nltk/nltk/tree/3.6.2/nltk/translate/gleu_score.py#L17">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Calculates the sentence level GLEU (Google-BLEU) score described in</p>
<blockquote>
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,
Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,
George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,
Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,
Jeffrey Dean. (2016) Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation.
eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf
Retrieved on 27 Oct 2016.</blockquote>
<dl class="rst-docutils">
<dt>From Wu et al. (2016):</dt>
<dd><dl class="rst-first rst-last rst-docutils">
<dt>"The BLEU score has some undesirable properties when used for single</dt>
<dd>sentences, as it was designed to be a corpus measure. We therefore
use a slightly different score for our RL experiments which we call
the 'GLEU score'. For the GLEU score, we record all sub-sequences of
1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then
compute a recall, which is the ratio of the number of matching n-grams
to the number of total n-grams in the target (ground truth) sequence,
and a precision, which is the ratio of the number of matching n-grams
to the number of total n-grams in the generated output sequence. Then
GLEU score is simply the minimum of recall and precision. This GLEU
score's range is always between 0 (no matches) and 1 (all match) and
it is symmetrical when switching output and target. According to
our experiments, GLEU score correlates quite well with the BLEU
metric on a corpus level but does not have its drawbacks for our per
sentence reward objective."</dd>
</dl>
</dd>
<dt>Note: The initial implementation only allowed a single reference, but now</dt>
<dd>a list of references is required (which is consistent with
bleu_score.sentence_bleu()).</dd>
</dl>
<p>The infamous "the the the ... " example</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>ref = <span class="py-string">'the cat is on the mat'</span>.split()
<span class="py-prompt">&gt;&gt;&gt; </span>hyp = <span class="py-string">'the the the the the the the'</span>.split()
<span class="py-prompt">&gt;&gt;&gt; </span>sentence_gleu([ref], hyp)  <span class="py-comment"># doctest: +ELLIPSIS</span>
<span class="py-output">0.0909...</span>
</pre></blockquote>
<p>An example to evaluate normal machine translation outputs</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>ref1 = <span class="py-builtin">str</span>(<span class="py-string">'It is a guide to action that ensures that the military '</span>
<span class="py-more">... </span>           <span class="py-string">'will forever heed Party commands'</span>).split()
<span class="py-prompt">&gt;&gt;&gt; </span>hyp1 = <span class="py-builtin">str</span>(<span class="py-string">'It is a guide to action which ensures that the military '</span>
<span class="py-more">... </span>           <span class="py-string">'always obeys the commands of the party'</span>).split()
<span class="py-prompt">&gt;&gt;&gt; </span>hyp2 = <span class="py-builtin">str</span>(<span class="py-string">'It is to insure the troops forever hearing the activity '</span>
<span class="py-more">... </span>           <span class="py-string">'guidebook that party direct'</span>).split()
<span class="py-prompt">&gt;&gt;&gt; </span>sentence_gleu([ref1], hyp1) <span class="py-comment"># doctest: +ELLIPSIS</span>
<span class="py-output">0.4393...</span>
<span class="py-prompt">&gt;&gt;&gt; </span>sentence_gleu([ref1], hyp2) <span class="py-comment"># doctest: +ELLIPSIS</span>
<span class="py-output">0.1206...</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">references:</span>list(list(str))</td><td class="fieldArgDesc">a list of reference sentences</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">hypothesis:</span>list(str)</td><td class="fieldArgDesc">a hypothesis sentence</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">min​_len:</span>int</td><td class="fieldArgDesc">The minimum order of n-gram this function should extract.</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">max​_len:</span>int</td><td class="fieldArgDesc">The maximum order of n-gram this function should extract.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">float</td><td class="fieldArgDesc">the sentence level GLEU score.</td></tr></table></div>
  </div>
</div>

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="http://www.nltk.org/" class="projecthome">nltk-3.6.2</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    22.2.2.dev0 at 2022-03-04 17:22:04.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>